{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXT2aDaMzGxxZjyRI4RX2A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umairimran/python-scraping/blob/master/githubScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "  !pip install -q streamlit\n",
        "  !npm install localtunnel\n",
        "  !wget -q -O - ipv4.icanhazip.com\n",
        "  !streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "E7hbN_tXoLi8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSqZeFD_oBg-",
        "outputId": "a0d4c2c4-14d6-4b50-c6cc-142223dc47d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJQmnTlzoKmm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scrape.py\n",
        "import streamlit as st\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from scrape import getData\n",
        "\n",
        "def getData(userName):\n",
        "   url = \"https://github.com/{}?tab=repositories\".format(userName)\n",
        "   page = requests.get(url)\n",
        "   decoded = page.content.decode(\"utf-8\") # Converting content into HTML\n",
        "   # Creating and saving html file\n",
        "   f = open(\"index.html\",'w')\n",
        "   f.write(decoded)\n",
        "   f.close()\n",
        "\n",
        "   soup = BeautifulSoup(page.content , 'html.parser')\n",
        "   info = {}\n",
        "\n",
        "   #full Name\n",
        "   try:\n",
        "      info['name'] = soup.find(class_ = 'vcard-fullname').get_text()\n",
        "   except:\n",
        "      info['name']= ''\n",
        "   #image\n",
        "   try:\n",
        "      info['image_url'] =  soup.find(class_ = 'avatar-user')['src']\n",
        "   except:\n",
        "      info['image_url'] = ''\n",
        "   #followers and following\n",
        "   try:\n",
        "      info['followers'] = soup.select_one(\"a[href*=followers]\").get_text().strip().split('\\n')[0]\n",
        "   except:\n",
        "      info['followers'] = ''\n",
        "   try:\n",
        "      info['following'] = soup.select_one(\"a[href*=following]\").get_text().strip().split('\\n')[0]\n",
        "   except:\n",
        "      info['following'] = ''\n",
        "   #location\n",
        "   try:\n",
        "      info['location'] = soup.select_one('li[itemprop*=home]').get_text().strip()\n",
        "   except:\n",
        "      info['location'] = ''\n",
        "\n",
        "   #url\n",
        "   try:\n",
        "      info['url'] =  soup.select_one('li[itemprop*=url]').get_text().strip()\n",
        "   except:\n",
        "      info['url'] = ''\n",
        "\n",
        "   #get Repositories as a dataframe\n",
        "   repos = soup.find_all(class_ = 'source')\n",
        "   repo_info = []\n",
        "\n",
        "   for repo in repos:\n",
        "      #repo name and link\n",
        "      try:\n",
        "         name =  repo.select_one('a[itemprop*=codeRepository]').get_text().strip()\n",
        "         link = 'https://github.com/{}/{}'.format(userName,name)\n",
        "      except:\n",
        "         name = ''\n",
        "         link = ''\n",
        "   #repo update time\n",
        "      try:\n",
        "         updated = repo.find('relative-time').get_text()\n",
        "      except:\n",
        "         updated = ''\n",
        "   # programming language\n",
        "      try:\n",
        "         language = repo.select_one('span[itemprop*=programmingLanguage]').get_text()\n",
        "      except:\n",
        "         language = ''\n",
        "   # description\n",
        "      try:\n",
        "         description = repo.select_one('p[itemprop*=description]').get_text().strip()\n",
        "      except:\n",
        "         description = ''\n",
        "      repo_info.append({'name': name ,\n",
        "      'link': link ,\n",
        "      'updated ':updated ,\n",
        "      'language': language ,\n",
        "      'description':description})\n",
        "   repo_info = pd.DataFrame(repo_info)\n",
        "   return info , repo_info\n",
        "\n",
        "userName = input(\"Enter Github user name:  \")\n",
        "info , repo_info = getData(userName)\n",
        "# Printing info\n",
        "print (\"info\")\n",
        "print(info)\n",
        "\n",
        "# Printing repo_info\n",
        "print(\"repo_info\")\n",
        "print(repo_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdq4NIUSoekT",
        "outputId": "eeb76774-f4cd-48f6-8bf5-0acd27372210"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scrape.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def getData(userName):\n",
        "   url = \"https://github.com/{}?tab=repositories\".format(userName)\n",
        "   page = requests.get(url)\n",
        "   decoded = page.content.decode(\"utf-8\") # Converting content into HTML\n",
        "   # Creating and saving html file\n",
        "   f = open(\"index.html\",'w')\n",
        "   f.write(decoded)\n",
        "   f.close()\n",
        "\n",
        "   soup = BeautifulSoup(page.content , 'html.parser')\n",
        "   info = {}\n",
        "\n",
        "   #full Name\n",
        "   try:\n",
        "      info['name'] = soup.find(class_ = 'vcard-fullname').get_text()\n",
        "   except:\n",
        "      info['name']= ''\n",
        "   #image\n",
        "   try:\n",
        "      info['image_url'] =  soup.find(class_ = 'avatar-user')['src']\n",
        "   except:\n",
        "      info['image_url'] = ''\n",
        "   #followers and following\n",
        "   try:\n",
        "      info['followers'] = soup.select_one(\"a[href*=followers]\").get_text().strip().split('\\n')[0]\n",
        "   except:\n",
        "      info['followers'] = ''\n",
        "   try:\n",
        "      info['following'] = soup.select_one(\"a[href*=following]\").get_text().strip().split('\\n')[0]\n",
        "   except:\n",
        "      info['following'] = ''\n",
        "   #location\n",
        "   try:\n",
        "      info['location'] = soup.select_one('li[itemprop*=home]').get_text().strip()\n",
        "   except:\n",
        "      info['location'] = ''\n",
        "\n",
        "   #url\n",
        "   try:\n",
        "      info['url'] =  soup.select_one('li[itemprop*=url]').get_text().strip()\n",
        "   except:\n",
        "      info['url'] = ''\n",
        "\n",
        "   #get Repositories as a dataframe\n",
        "   repos = soup.find_all(class_ = 'source')\n",
        "   repo_info = []\n",
        "\n",
        "   for repo in repos:\n",
        "      #repo name and link\n",
        "      try:\n",
        "         name =  repo.select_one('a[itemprop*=codeRepository]').get_text().strip()\n",
        "         link = 'https://github.com/{}/{}'.format(userName,name)\n",
        "      except:\n",
        "         name = ''\n",
        "         link = ''\n",
        "   #repo update time\n",
        "      try:\n",
        "         updated = repo.find('relative-time').get_text()\n",
        "      except:\n",
        "         updated = ''\n",
        "   # programming language\n",
        "      try:\n",
        "         language = repo.select_one('span[itemprop*=programmingLanguage]').get_text()\n",
        "      except:\n",
        "         language = ''\n",
        "   # description\n",
        "      try:\n",
        "         description = repo.select_one('p[itemprop*=description]').get_text().strip()\n",
        "      except:\n",
        "         description = ''\n",
        "      repo_info.append({'name': name ,\n",
        "      'link': link ,\n",
        "      'updated ':updated ,\n",
        "      'language': language ,\n",
        "      'description':description})\n",
        "   repo_info = pd.DataFrame(repo_info)\n",
        "   return info , repo_info\n",
        "\n",
        "st.title('Github Scraper')\n",
        "userName=st.text_input(\"Enter Github user name:  \")\n",
        "if userName!='':\n",
        "  try:\n",
        "      info , repo_info = getData(userName)\n",
        "      for key,value in info.items():\n",
        "        if key!='image_url':\n",
        "          st.subheader(\n",
        "            '''\n",
        "           {} : {}\n",
        "          '''.format(key, value)\n",
        "          )\n",
        "        else:\n",
        "          st.image(value)\n",
        "      st.subheader('Repositories')\n",
        "      st.table(repo_info)\n",
        "\n",
        "  except:\n",
        "    st.write(\"User Dont Exist\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6qRaStTuLOF",
        "outputId": "feb32949-c562-47ca-9698-4614da938a37"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rje4IxLevf8w"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_56Hu5nwpfpM",
        "outputId": "84010014-3825-43a4-e4a2-06941f1b9438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.52s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h35.196.226.252\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.503s\n",
            "your url is: https://nasty-shoes-wait.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-3L1zjLKxAhH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}